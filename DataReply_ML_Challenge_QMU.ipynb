{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reply - QMU ML Challenge\n",
    "## Introduction\n",
    "\n",
    "In this challenge, you will be using US census data to predict whether or not a person earns over $50k a year.\n",
    "This challenge takes you through the data science process of Data Loading, Data Exploration & Cleansing, Feature Generation, Model Calibration and Model Evaluation. You will build different models to tackle this classification problem, evaluating your models along the way, and finally combining each of your models into your own ensemble classifier.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DS process\n",
    "\n",
    "There are 5 main steps in the data science process. \n",
    "\n",
    "1. Data loading. \n",
    "2. Data exploration and clensing. \n",
    "3. Feature generation.\n",
    "4. Model calibration.\n",
    "5. Model evaluation.\n",
    "\n",
    "The majority of a data scientists time is spend on number 2. Exploration and clensing are a very important step in the data science process with data scientists typically spending 80% of their time preparing the data.\n",
    "\n",
    "We will be following this process to demonstrate how ML projects are developped. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading\n",
    "\n",
    "In this example, we are using a relatively small and static dataset - so the data loading process is straight forward. \n",
    "\n",
    "In real-life projects, the data:\n",
    " - is sometimes dynamic - updated regularly, or streaming into the data storage system in real time\n",
    " - is often very large - 100s of GBs of TBs\n",
    " - is ALWAYS dirty, and needs cleaning\n",
    "\n",
    "In these cases, some specialised big data or streaming tools may be needed to develop the solution, such as spark, kafke, etc. But, for this example, we will consider this static dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Read the data into a dataframe using pandas, passing the argument names = [...] into the read_csv function to set the column names.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = ZipFile('census_data.zip')\n",
    "df = pd.read_csv(zip_file.open('census_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can inspect a sample of the dataframe by using the df.head( ) function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You will notice that some of the columns are obscured from view. This is a default setting and can be overwritten using the following command.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Data Exploration & Clensing\n",
    "\n",
    "*Now that the data is loaded into a dataframe, we can look at some summary statistics of numerical columns to give us a first idea of the data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Different features of the data may need different preprocessing steps. For example, \"sex\" might be encoded to 0 and 1 to represent male and female\\*, but doing this with \"wage_per_hour\" - a continuous data type - wouldn't make sense.*\n",
    "\n",
    "\\**NOTE: the US census dataset used here uses biological \"sex\", for which there were 2 options (male and female), as opposed to gender where may be more.*\n",
    "\n",
    "*There are 4 different 'types' of feature which may be present in the data: nominal, ordinal, interval and ratio. Each of these should be dealt with in different ways. [This website](https://www.graphpad.com/support/faq/what-is-the-difference-between-ordinal-interval-and-ratio-variables-why-should-i-care/) gives some more context on these different feature types.*\n",
    "\n",
    "*From these definitions, we can see that \"sex\" is obviously a nominal feature. If there are other nominal features in the dataset, it might be useful to create some kind of function which can be applied to each nominal column - then also for each ordinal, interval and ratio columns. To do this we shall make use of sklearn's transformers - more on these later.*\n",
    "\n",
    "*But first, we need to determine which features belong to each of these categories.*\n",
    "\n",
    "*We can inspect the columns and their datatypes by running df.dtypes in the next cell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "As we can see, our features are either int64, or object datatypes. (Here, object can be thought of as string.)\n",
    "\n",
    "Inspecting columns of each datatype in turn might be a good place to start when determining which columns are nominal, etc. \n",
    "Remember though, ALL DATA IS DIRTY. \n",
    "Columns may need to be changed from one datatype to another, this is just a starting point.\n",
    "\n",
    "\n",
    "Complete the function in the next cell to return a list of all columns from a dataframe of a given datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols_of_type(input_df, data_type):\n",
    "    '''\n",
    "    Takes a dataframe and dtype, eg. string, and returns a list of columns names of that dtype.\n",
    "    '''\n",
    "    output_list = []\n",
    "    ### fill in logic here.\n",
    "    ### fill in logic here.\n",
    "    ### fill in logic here.    \n",
    "    \n",
    "    return output_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display int datatypes from our dataframe\n",
    "get_cols_of_type(df, int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display object datatypes from our dataframe\n",
    "get_cols_of_type(df, object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Now let's go through each column and decide which category each belongs to. Then we can deal with each feature type in turn by passing our list of columns into our transformer function.*\n",
    "\n",
    "**Remember** - earn_over_50k will be our target variable, so we want to keep that separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through the features in the data and fill in the lists of columns for each feature type.\n",
    "\n",
    "nominal_cols = []\n",
    "\n",
    "ordinal_cols = []\n",
    "\n",
    "interval_cols = []\n",
    "\n",
    "ratio_cols = []\n",
    "\n",
    "target_col = [\"earn_over_50k\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: something which might be useful to inspect what values a feature can take is df.column.unique( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Once you have finished, perform a quick check that the number of columns in the dataframe matches the number of elements in all your lists combined, using 'assert'.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(nominal_cols+ordinal_cols+interval_cols+ratio_cols+target_col) == len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Now let's look at each feature type in turn and see how we will deal with them, starting with nominal features. \n",
    "The following for look allows us to inspect all the possbile values which each nominal feature can take. *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in nominal_cols:\n",
    "    print(feature+\":\")\n",
    "    print(sorted(df[feature].unique()))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*One of the first things to notice is that for the country_of_birth_X columns, we have some missing values represented by \"?\". Dealing with missing values is a common thing to have to do in any data science project. Some typical methods for dealing with missing numerical values include mean imputation and median imputation. *\n",
    "\n",
    "**Think of a way to impute the missing values in this case for the categorical columns, country_of_birth_X. ([Hint](https://pandas.pydata.org/pandas-docs/stable/reference/series.html))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in [\"country_of_birth_father\", \"country_of_birth_mother\", \"country_of_birth_self\"]:\n",
    "    ### fill in the following line with some logic to replace \"?\" by the modal country of birth\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now if we look again at the values these three features can take, we should see that \"?\" is no longer an option.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in nominal_cols:\n",
    "    print(feature+\":\")\n",
    "    print(sorted(df[feature].unique()))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notice that some form of \"Not in universe\" comes up a lot for missing values in the different fields. When we use transformers later on, it would be useful to be able to handle all of these in the same way.*\n",
    "\n",
    "**So as another preprocessing step, change all instances containing \"Not in universe\" to \"null\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(\"Not in universe\", \"null\").replace(\"Not in universe or children\", \"null\")\\\n",
    "                                          .replace(\"Not in universe under 1 year old\", \"null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in nominal_cols:\n",
    "    print(feature+\":\")\n",
    "    print(sorted(df[feature].unique()))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Feature generation\n",
    "*Now that we see all the possible values which each nominal feature can take, we need to decide on how to handle them.*\n",
    "\n",
    "*For some features, we may wish to convert them into binary variables, such as own_business_or_self_employed, yes or not yes.*\n",
    "\n",
    "*For others, such as for each country_of_birth_{}, we might wish to binarize these to something like from \"United-States\", yes or no?*\n",
    "\n",
    "*Other features should not be binarized, such as marital_status, and we will have to use [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).*\n",
    "\n",
    "**Come up with a strategy to encode the following columns as binary variables:**\n",
    "\\[ sex, live_in_this_house_1_year_ago,  country_of_birth_father,  country_of_birth_mother,  country_of_birth_self,  own_business_or_self_employed \\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_male\"] = np.where(df['sex']==\"Male\", 1, 0)\n",
    "df[\"is_lived_in_this_house_1_year_ago\"]= ### Fill in logic\n",
    "df[\"born_usa_father\"] = ### Fill in logic\n",
    "df[\"born_usa_mother\"] = ### Fill in logic\n",
    "df[\"born_usa_self\"]   = ### Fill in logic\n",
    "df[\"is_self_employed\"]= ### Fill in logic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now drop the columns we no longer need.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"sex\", \"live_in_this_house_1_year_ago\", \"country_of_birth_father\",\n",
    "            \"country_of_birth_mother\", \"country_of_birth_self\", \"own_business_or_self_employed\"]\n",
    "df = df.drop(columns = drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we have manually dealt with some of the nominal_cols, we can disguard them from the list of nominal columns we were dealing with. To do this, we make use of sets.*\n",
    "\n",
    "*The remaining nominal_cols will need to be one-hot encoded.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_nominal_cols = list(set(nominal_cols)-set(drop_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_nominal_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Ordinal features\n",
    "*To deal with ordinal values, we need to do a little more.\n",
    "First we need to create an order and map that order to increasing integers which represent that order. Let's take a look at the values education can take.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df.education.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clearly there is some kind of order to this. Completing 10th grade is better than completing 9th grade, having a phd is better than having a masters, etc.*\n",
    "\n",
    "*Let's try and put some kind of order to the elements in that list in the form of a dictionary, with the least advanced education starting at 0, and the most advanced education being the greatest number.*\n",
    "\n",
    "*The benefit of the dictionary is that different keys can have the same value, thus if you think 2 different educations are equal, you can assign them the same value.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete the dictionary to reasonably encapsulate the order in the data\n",
    "education_dict = {'Children':0,\n",
    "                 'Less than 1st grade':1 \n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we can use the [pandas.Series.map](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html) function to map these categorical values to numerically ordered ones.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"education\"] = df[\"education\"].map(education_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Ratio features\n",
    "*We will, however, be considering the ratio columns. Given that the ratio columns have different scales which can skew the model massively in favour of a particular feature, we should apply some technique to mitigate this. \n",
    "A typical technique to use is to scale each of the features so that they are on the same scale, usualy the interval [0,1].\n",
    "To do this we can use min-max scaler in sklearn.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Target feature\n",
    "\n",
    "*For the rest of the notebook and analysis, we shall keep the target variable separate from the other variables. We will also transform this variable to a binary varable with 1 representing those earning over 50k.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.where(df[\"earn_over_50k\"]==\"-50000\", 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining all the features together with transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the columns we binarized earlier with custom conditions\n",
    "bin_cols=[\"education\", \"is_male\", \"is_lived_in_this_house_1_year_ago\", \"born_usa_father\",\n",
    "                      \"born_usa_mother\", \"born_usa_self\", \"is_self_employed\"]\n",
    "\n",
    "cols = ratio_cols+bin_cols+remaining_nominal_cols\n",
    "\n",
    "data = df[cols]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we can see the dataframe with numeric features to the left, and our nominal features to the right.\n",
    "The next step is to utilise the Scikit learn's ColumnTransformer and one-hot encoder to finish the pre-processing of the data frame.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_features = ratio_cols+bin_cols\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "                        ('imputer', SimpleImputer(strategy='median')),\n",
    "                        ('scaler', MinMaxScaler())])\n",
    "\n",
    "categorical_features = remaining_nominal_cols\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='null')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = preprocessor.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notice 2 things:*\n",
    "1. *the number of features after proprocessing has increased substantially from 22 to 92 - this is because of the one hot encoding creating binary variables for each possible category.*\n",
    "2. *the data is now stored as a sparse matrix. Sparse matrices are a more efficient way of storing data which is mostly 0s but contains a few non-zero values.*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick note on overfitting\n",
    "*In this section, we will illustrate the notion of overfitting. To do this we will first generate some synthetic data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from sklearn.externals.six import StringIO\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    " \n",
    "# Create a random dataset\n",
    "rng = np.random.RandomState(42) # fix the seed so the result are stable\n",
    "N_points = 400\n",
    "X = np.sort(5 * rng.rand(N_points, 1), axis=0)\n",
    "y = np.sin(X).ravel() + .4 * (0.5 - rng.rand(N_points))\n",
    "\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.xlabel('Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*So, above is our example dataset and suppose we want fit a decision tree regressor to estimate the value of unseen data. (You don't need to know anything about decision trees at this stage. But if you want to find out more, you can read more about decision trees [here](http://scikit-learn.org/stable/modules/tree.html).) All you need to know is that max_depth is a hyperparameter of the decision tree and if we increase that hyperparameter, the tree can model more complexity in the data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interactive, IntSlider\n",
    "from sklearn import tree\n",
    "\n",
    "def train_and_plot(max_depth):\n",
    "    est = tree.DecisionTreeRegressor(max_depth=max_depth)\n",
    "    est.fit(X, y)\n",
    "\n",
    "    plt.plot(X, y, 'b.', label='data')\n",
    "    line = plt.plot(X, est.predict(X), 'r-', label='model')\n",
    "    plt.setp(line, linewidth=3.)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend(loc='upper right');\n",
    "    plt.show()\n",
    "    \n",
    "max_depth_slider=IntSlider(min=1,max=8,step=1,value=2)\n",
    "interactive(train_and_plot, max_depth=max_depth_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Try playing around with `max_depth` in the above above cell. What happens when you set it to `max_depth = 2`.  Does the fitted model look better or worse?  What about `4` or `8`?  We can tell (visually) that `max_depth = 2` is *underfitting* and `max_depth = 8` is *overfitting*. But how can we do this computationally?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In-sample (Training) error\n",
    "*Naively, you might think that we could just measure the error of the model and choose the model with the best error.  For example, let's define the error as the mean squared error (MSE).  Let's try that below:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "max_depths = range(1, 10)\n",
    "in_sample_errors = []\n",
    "for max_depth in max_depths:\n",
    "    y_pred = tree.DecisionTreeRegressor(max_depth=max_depth).fit(X, y).predict(X)\n",
    "    in_sample_errors.append(metrics.mean_squared_error(y, y_pred))\n",
    "    \n",
    "plt.plot(max_depths, in_sample_errors, label='In-Sample Error')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can see that the error is decreasing with the depth of the tree. Our visual inspection told us models with max_depth = 8 completely over-fit the error. It turns out that the In-Sample Error that we calculated above will always decrease with the complexity of the model (in this case, the depth of the tree). We can see from above that this leads us to overfit the data. In order to test how well our model generalizes, we need to see how it performs on new data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out of sample (Testing) error\n",
    "\n",
    "*One way to do this is to (randomly) split the data into training and test sets.  We train on the training set and test the resulting model on the test set.  Since the trained model never saw the test data, we can evaluate the performance on the test data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errors = []\n",
    "for max_depth in max_depths:\n",
    "    est = tree.DecisionTreeRegressor(max_depth=max_depth).fit(X_train, y_train)\n",
    "    y_pred = est.predict(X_test)\n",
    "    test_errors.append(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "plt.plot(max_depths, in_sample_errors, label='In-Sample Error')\n",
    "plt.plot(max_depths, test_errors, label='Out-of-Sample Error')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the picture we were looking for!  The *In-Sample Error (Training)* is always decreasing but the out *Out-of-Sample Error (Testing)* initially decreases with model complexity (higher `max_depth`) but ultimately increases again.  The *Out-of-Sample Error (Testing)* is the metric to look at when evaluating overfitting.\n",
    "\n",
    "It turns out that this is an illustration of a very general problem in machine-learning called **Bias-Variance tradeoff** (the concept is so general that it even has a [Wikipedia article](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_dilemma).  A more rigorous account can be found [here](http://www.brnt.eu/phd/node14.html)).  The tradeoff tells us that we can decompose our *Out-of-Sample Error* into\n",
    "\n",
    "$$ \\mbox{Out-of-Sample Error} = \\mbox{Bias} + \\mbox{Variance}. $$\n",
    "\n",
    "The *Bias* corresponds to how far off we expect the model to deviate from reality (i.e. the model's bias) because of parametric assumptions (e.g. we forced the model to be linear or to be a tree of maximum depth 2).  It is given by the *In-Sample Error* of the above plot and always goes down with complexity.  High Bias models correspond to *underfitting*.\n",
    "\n",
    "The *Variance* accounts for the fact that the model was only trained on a (noisy) subset of the data and that the idiosyncratic noise in the data is therefore likely to contribute some variance to the model.  The more complex we allow the model to be, the more likely we are to overfit by picking up more of this noise.  High variance models correspond to *overfitting*.\n",
    "\n",
    "We can also think of bias as unmodeled data and variance as modeled noise.  As we increase the complexity of the model, we will necessarily model more of the data (reduce bias, reduce underfitting) but also start modeling noise (increase variance, increase overfitting).  Here's a helpful diagram of the decomposition.  Notice that at the optimal point, we have not yet learned on all our signal (still unmodeled data left) and we have picked up some noise and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. and 5. - Model Calibration and Evaluation\n",
    "\n",
    "Back to the problem at hand and it's over to you... \n",
    "\n",
    "Split the preprocessed data and target values into a training and test data set using train_test_split. \n",
    "Then train the model of your choice and evaluate the outcome. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "### import the classifier of your choice.\n",
    "\n",
    "### complete the pipeline with classifier\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', ... )])\n",
    "\n",
    "### complete the splitting of your data\n",
    "X_train, X_test, y_train, y_test = train_test_split(...)\n",
    "\n",
    "### evaluate your classifier\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    " - How accurate is the model? \n",
    " - Is accuracy the best metric? What others can you think of? ([Hint](https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c))\n",
    " - What is the baseline accuracy that your model should be compared against?\n",
    " - How might you deal with unbalanced classes in your classification problem? ([Hint](https://elitedatascience.com/imbalanced-classes))\n",
    " - Can you add a grid search to improve your model performance without overfitting? ([Hint](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#using-the-prediction-pipeline-in-a-grid-search))\n",
    " - How do other models compare to the one you made?\n",
    " - What is the best model you managed to come up with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge:**\n",
    "\n",
    " - Create your own ensemble model by combining the predictions from the previous models you have built."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
